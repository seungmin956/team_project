# utils/chat_recall.py - LLM ê¸°ë°˜ ìˆ˜ì¹˜í˜• ì§ˆë¬¸ ì²˜ë¦¬ í†µí•© ë²„ì „
import sys
import pysqlite3
sys.modules['sqlite3'] = pysqlite3
import json
import os
from datetime import datetime, timedelta
from typing import TypedDict, List, Dict, Any
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.messages import AIMessage, HumanMessage
from langgraph.graph import StateGraph, START, END
from langchain_teddynote import logging
from utils.fda_realtime_crawler import get_crawler, update_vectorstore_with_new_data
from utils.google_crawler import search_and_extract_news_with_fallback, format_news_for_context

# ğŸ†• ëª¨ë“ˆí™”ëœ import
from utils.processors.filter_criteria_extractor import FilterCriteriaExtractor
from utils.processors.numerical_processor import NumericalQueryProcessor
from utils.processors.logical_processor import LogicalQueryProcessor
from utils.prompts.recall_prompts import RecallPrompts, TranslationPrompts
from utils.fda_realtime_crawler import get_crawler, update_vectorstore_with_new_data
from utils.google_crawler import search_and_extract_news_with_fallback, format_news_for_context


# LLM ê¸°ë°˜ ìˆ˜ì¹˜í˜• ì§ˆë¬¸ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ì— í•„ìš”í•œ library import
import re
from collections import Counter

load_dotenv()
logging.langsmith("LLMPROJECT")

class RecallState(TypedDict):
    """ë¦¬ì½œ ê²€ìƒ‰ ì‹œìŠ¤í…œ ìƒíƒœ"""
    question: str
    question_en: str  # ì˜ì–´ ë²ˆì—­ëœ ì§ˆë¬¸
    recall_context: str
    recall_documents: List[Document]
    final_answer: str
    chat_history: List[HumanMessage | AIMessage]
    news_context: str  # êµ¬ê¸€ ë‰´ìŠ¤ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    news_documents: List[Dict]  # ë‰´ìŠ¤ ë¬¸ì„œë“¤ ì¶”ê°€
    filter_conditions: Dict[str, Any]  # ğŸ†• í•„í„°ë§ ì¡°ê±´ ì¶”ê°€


## ChromaDB ì´ˆê¸°í™” í™•ì¸ í•¨ìˆ˜
def verify_chromadb_setup(vectorstore) -> bool:
    """ChromaDB ì„¤ì • ë° ë©”íƒ€ë°ì´í„° êµ¬ì¡° í™•ì¸"""
    if not vectorstore:
        print("âŒ ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    
    try:
        # ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸
        collection = vectorstore._collection
        doc_count = collection.count()
        print(f"âœ… ChromaDB ì—°ê²° ì„±ê³µ: {doc_count}ê°œ ë¬¸ì„œ")
        
        if doc_count > 0:
            # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
            sample_data = collection.get(limit=1, include=["metadatas"])
            if sample_data and sample_data.get('metadatas'):
                sample_metadata = sample_data['metadatas'][0]
                print(f"ğŸ“‹ ë©”íƒ€ë°ì´í„° í•„ë“œ: {list(sample_metadata.keys())}")
                
                # ont_ í•„ë“œ í™•ì¸
                ont_fields = [key for key in sample_metadata.keys() if key.startswith('ont_')]
                if ont_fields:
                    print(f"ğŸ¯ ont_ í•„ë“œ ë°œê²¬: {ont_fields}")
                else:
                    print("âš ï¸ ont_ ë©”íƒ€ë°ì´í„° í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ì¹˜ ë¶„ì„ì— ì œí•œì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                
                return True
            else:
                print("âš ï¸ ë©”íƒ€ë°ì´í„°ê°€ ì—†ëŠ” ë¬¸ì„œë“¤ì…ë‹ˆë‹¤.")
                return False
        else:
            print("âš ï¸ ChromaDBì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
    except Exception as e:
        print(f"âŒ ChromaDB í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def initialize_recall_vectorstore():
    """ì´ ì½”ë“œëŠ” ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤"""
    persist_dir = "./data/chroma_db_recall"
    
    if os.path.exists(persist_dir) and os.listdir(persist_dir):
        try:
            print("ê¸°ì¡´ ë¦¬ì½œ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
            embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
            
            vectorstore = Chroma(
                persist_directory=persist_dir,
                embedding_function=embeddings,
                collection_name="FDA_recalls"
            )
            
            collection = vectorstore._collection
            doc_count = collection.count()
            print(f"ë¦¬ì½œ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ ({doc_count}ê°œ ë¬¸ì„œ)")
            # ğŸ¯ ê²€ìƒ‰ ì‹œ ìµœì‹  ë°ì´í„° ìš°ì„ , í¬ë¡¤ë§ìœ¼ë¡œ ìµœì‹  ë°ì´í„° ë³´ê°•
            return vectorstore
                
        except Exception as e:
            print(f"ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    else:
        print("âŒ ë²¡í„°ìŠ¤í† ì–´ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        return None

# ì „ì—­ ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”
try:
    recall_vectorstore = initialize_recall_vectorstore()
except Exception as e:
    print(f"ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    recall_vectorstore = None

def translation_node(state: RecallState) -> RecallState:
    """ì¡°ê±´ë¶€ ë²ˆì—­ ë…¸ë“œ - ê³ ìœ ëª…ì‚¬ ë³´ì¡´ ë²ˆì—­"""
    
    will_use_vectorstore = recall_vectorstore is not None
    
    if will_use_vectorstore:
        # ê³ ìœ ëª…ì‚¬ ë³´ì¡´ ë²ˆì—­ ìˆ˜í–‰
        question_en = translate_with_proper_nouns(state["question"])
        print(f"ğŸ”¤ ê³ ìœ ëª…ì‚¬ ë³´ì¡´ ë²ˆì—­: '{state['question']}' â†’ '{question_en}'")
    else:
        question_en = state["question"]
        print(f"ğŸ”¤ ë²ˆì—­ ìƒëµ (ì›¹ ê²€ìƒ‰ ì „ìš©): '{question_en}'")
    
    # ê²€ìƒ‰ìš© í‚¤ì›Œë“œ ì¶”ì¶œ
    search_keywords = extract_question_keywords(state["question"])
    
    return {
        **state,
        "question_en": question_en,
        "search_keywords": search_keywords
    }

def translate_with_proper_nouns(korean_text: str) -> str:
    """ê³ ìœ ëª…ì‚¬ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ë²ˆì—­í•˜ëŠ” ê°œì„ ëœ í•¨ìˆ˜"""
    try:
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.1)
        
        # ğŸ”§ ëª¨ë“ˆí™”ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        prompt_template = TranslationPrompts.PROPER_NOUN_TRANSLATION
        prompt = prompt_template.format(korean_text=korean_text)
        
        response = llm.invoke([HumanMessage(content=prompt)])
        translated = response.content.strip()
        
        # ë²ˆì—­ ê²°ê³¼ ê²€ì¦ ë° í›„ì²˜ë¦¬
        if translated and len(translated) > 0:
            translated = translated.replace('"', '').replace("'", "")
            if translated.lower().startswith('translation:'):
                translated = translated[12:].strip()
            return translated
        else:
            return korean_text
            
    except Exception as e:
        print(f"ê³ ìœ ëª…ì‚¬ ë³´ì¡´ ë²ˆì—­ ì˜¤ë¥˜: {e}")
        return korean_text


def is_recall_related_question(question: str) -> bool:
    """ì§ˆë¬¸ì´ ë¦¬ì½œ ê´€ë ¨ì¸ì§€ íŒë‹¨í•˜ëŠ” í•¨"""
    recall_keywords = [
        "ë¦¬ì½œ", "íšŒìˆ˜", "recall", "withdrawal", "safety alert",
        "FDA", "ì‹í’ˆì•ˆì „", "ì œí’ˆ ë¬¸ì œ", "ì˜¤ì—¼", "contamination",
        "ì„¸ê· ", "bacteria", "E.coli", "salmonella", "listeria",
        "ì•Œë ˆë¥´ê¸°", "allergen", "ë¼ë²¨ë§", "labeling",
        "ì‹ì¤‘ë…", "ì•ˆì „", "ìœ„í—˜", "ë¬¸ì œ", "ì‚¬ê³ "
    ]
    
    question_lower = question.lower()
    return any(keyword.lower() in question_lower for keyword in recall_keywords)

def recall_search_node(state: RecallState) -> RecallState:
    """ë²¡í„°DBì—ì„œ ë¦¬ì½œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ì‹¤ì‹œê°„ í¬ë¡¤ë§ì„ ì¡°ê±´ë¶€ë¡œ ìˆ˜í–‰"""
    
    # ì¼ë°˜ ì§ˆë¬¸ì´ë©´ ê²€ìƒ‰ ìƒëµ
    if not is_recall_related_question(state["question"]):
        print(f"ì¼ë°˜ ì§ˆë¬¸ ê°ì§€ - ë¦¬ì½œ ê²€ìƒ‰ ìƒëµ")
        return {
            **state,
            "recall_context": "",
            "recall_documents": []
        }
    
    if recall_vectorstore is None:
        print("ë¦¬ì½œ ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return {
            **state,
            "recall_context": "",
            "recall_documents": []
        }
    
    try:
        # í•œ ë²ˆë§Œ ì •ì˜ - ì‹¤ì‹œê°„ í¬ë¡¤ë§ê³¼ ê²€ìƒ‰ ì „ëµ ëª¨ë‘ì— ì‚¬
        chat_history = state.get("chat_history", [])
        recent_keywords = ["ìµœê·¼","ìµœì‹ ","ê·¼ë˜", "recent", "latest", "new", "ìƒˆë¡œìš´", "ìš”ì¦˜", "í˜„ì¬"]
        is_recent_query = any(keyword in state["question"].lower() for keyword in recent_keywords)
        
        # ì„¸ì…˜ ë‚´ì—ì„œ ì´ë¯¸ í¬ë¡¤ë§í–ˆëŠ”ì§€ í™•ì¸
        has_crawled_in_session = False
        for msg in chat_history:
            if isinstance(msg, AIMessage) and "âš¡ì‹¤ì‹œê°„:" in msg.content:
                has_crawled_in_session = True
                break
        
        # ì‹¤ì‹œê°„ í¬ë¡¤ë§ ì¡°ê±´: ìµœì‹  ë°ì´í„° ìš”ì²­ + ì„¸ì…˜ ë‚´ ë¯¸í¬ë¡¤ë§
        should_crawl = is_recent_query and not has_crawled_in_session
        
        if should_crawl:
            print("ğŸ” ìµœì‹  ë°ì´í„° ìš”ì²­ - ì‹¤ì‹œê°„ í¬ë¡¤ë§ ìˆ˜í–‰")
            try:     
                crawler = get_crawler()
                new_recalls = crawler.crawl_latest_recalls(days_back=7)
                
                if new_recalls:
                    added_count = update_vectorstore_with_new_data(new_recalls, recall_vectorstore)
                    print(f"âœ… ìƒˆ ë°ì´í„° {added_count}ê±´ ì¶”ê°€ë¨")
                else:
                    print("ğŸ“‹ ìƒˆ ë¦¬ì½œ ë°ì´í„° ì—†ìŒ")
                    
            except Exception as e:
                print(f"âš ï¸ ì‹¤ì‹œê°„ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
        # ê²€ìƒ‰ ì „ëµ ê²°ì •
        if is_recent_query:
            print("ğŸ“… ìµœì‹  ë°ì´í„° ìš°ì„  ê²€ìƒ‰ (ë‚ ì§œ ì •ë ¬ ìˆ˜í–‰)...")
            
            # ì „ì²´ ë°ì´í„° ë‚ ì§œ ì •ë ¬ ë¡œì§
            all_data = recall_vectorstore.get()
            all_documents = []

            for i, metadata in enumerate(all_data.get('metadatas', [])):
                if metadata:
                    content = all_data.get('documents', [])[i] if i < len(all_data.get('documents', [])) else ""
                    doc = Document(page_content=content, metadata=metadata)
                    all_documents.append(doc)

            def get_date_for_sorting(doc):
                date_str = doc.metadata.get('effective_date', '1900-01-01')
                try:
                    return datetime.strptime(date_str, '%Y-%m-%d')
                except:
                    return datetime(1900, 1, 1)

            # ì²­í¬ ì œê±° í›„ ë‚ ì§œìˆœ ì •ë ¬
            if len(all_documents) > 0 and 'chunk_index' in all_documents[0].metadata:
                url_groups = {}
                for doc in all_documents:
                    url = doc.metadata.get('url', 'unknown')
                    if url not in url_groups or len(doc.page_content) > len(url_groups[url].page_content):
                        url_groups[url] = doc
                unique_recalls = list(url_groups.values())
                unique_recalls.sort(key=get_date_for_sorting, reverse=True)
            else:
                unique_recalls = all_documents
                unique_recalls.sort(key=get_date_for_sorting, reverse=True)

            selected_docs = unique_recalls[:5]
            
        else:
            print("ğŸ¯ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰ (ìœ ì‚¬ë„ + í•„í„°ë§)...")
            
            search_query = state.get("question_en", state["question"])
            # í‚¤ì›Œë“œ ê¸°ë°˜ ë©”íƒ€ë°ì´í„° í•„í„° ìƒì„±
            question_keywords = extract_question_keywords(state["question"]).lower()
            
            selected_docs = recall_vectorstore.similarity_search(
                search_query, 
                k=10,  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ í›„ì²˜ë¦¬
                filter={"document_type": "recall"}
            )
            
            # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í›„ì²˜ë¦¬ í•„í„°ë§
            filtered_docs = []
            for doc in selected_docs:
                ont_food = doc.metadata.get('ont_food', '').lower()
                if question_keywords in ont_food or ont_food in question_keywords:
                    filtered_docs.append(doc)
            
            # í•„í„°ë§ëœ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ê²°ê³¼ ì‚¬ìš©
            selected_docs = filtered_docs[:5] if filtered_docs else selected_docs[:5]

        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ¯ ìµœì¢… selected_docs:")
        for i, doc in enumerate(selected_docs):
            date = doc.metadata.get('effective_date', 'N/A')
            title = doc.metadata.get('title', '')[:50]
            source = doc.metadata.get('source', '')
            print(f"  {i+1}. {date} | {source} | {title}...")

        context_parts = []
        for doc in selected_docs:
            content_with_meta = f"{doc.page_content}\nSource URL: {doc.metadata.get('url', 'N/A')}"
            context_parts.append(content_with_meta)

        context = "\n\n---\n\n".join(context_parts)
        
        print(f"ğŸ“Š ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(selected_docs)}ê±´")
        
        return {
            **state,
            "recall_context": context,
            "recall_documents": selected_docs
        }
        
    except Exception as e:
        print(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return {
            **state,
            "recall_context": "",
            "recall_documents": []
        }

def filtered_search_node(state: RecallState) -> RecallState:
    """í•„í„° ì¡°ê±´ì„ í™œìš©í•œ ì •í™•í•œ ChromaDB ê²€ìƒ‰ - ë””ë²„ê¹… ê°•í™”"""
    if recall_vectorstore is None:
        print("ë¦¬ì½œ ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return {
            **state,
            "recall_context": "",
            "recall_documents": []
        }
    
    try:
        # ğŸ†• ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
        print(f"ğŸ”§ filtered_search_node ì‹œì‘")
        print(f"ğŸ”§ ì „ì²´ state í‚¤ë“¤: {list(state.keys())}")
        
        filter_conditions = state.get("filter_conditions", {})
        print(f"ğŸ”§ ì¶”ì¶œëœ í•„í„° ì¡°ê±´: {filter_conditions}")
        print(f"ğŸ”§ í•„í„° ì¡°ê±´ íƒ€ì…: {type(filter_conditions)}")
        print(f"ğŸ”§ í•„í„° ì¡°ê±´ ê¸¸ì´: {len(filter_conditions) if filter_conditions else 0}")
        
        if not filter_conditions:
            print("âš ï¸ í•„í„° ì¡°ê±´ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜")
            return recall_search_node(state)
        
        # ChromaDB í•„í„° í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        extractor = FilterCriteriaExtractor(recall_vectorstore)
        chroma_filter = extractor.convert_to_chroma_filter(filter_conditions)
        
        # ë¬¸ì„œ íƒ€ì… í•„í„° ì¶”ê°€
        chroma_filter["document_type"] = "recall"
        
        # í•„í„°ë§ëœ ê²€ìƒ‰ ìˆ˜í–‰
        search_query = state.get("question_en", state["question"])
        
        print(f"ğŸ” ChromaDB í•„í„°ë§ ê²€ìƒ‰: query='{search_query}', filter={chroma_filter}")
        
        # ğŸ†• OR ì¡°ê±´ ì²˜ë¦¬ ê°œì„ 
        try:
            filtered_docs = recall_vectorstore.similarity_search(
                search_query,
                k=10,
                filter=chroma_filter
            )
        except Exception as search_error:
            print(f"âš ï¸ í•„í„°ë§ ê²€ìƒ‰ ì‹¤íŒ¨: {search_error}")
            # OR ì¡°ê±´ ë¬¸ì œì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¨ìˆœí™”ëœ í•„í„°ë¡œ ì¬ì‹œë„
            simple_filter = {"document_type": "recall"}
            for field, value in filter_conditions.items():
                if not isinstance(value, list):
                    simple_filter[field] = value
            
            print(f"ğŸ”„ ë‹¨ìˆœí™”ëœ í•„í„°ë¡œ ì¬ì‹œë„: {simple_filter}")
            filtered_docs = recall_vectorstore.similarity_search(
                search_query,
                k=10,
                filter=simple_filter
            )
        
        # ë‚˜ë¨¸ì§€ ê¸°ì¡´ ë¡œì§...
        if not filtered_docs:
            print("âŒ í•„í„°ë§ëœ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ. ì¡°ê±´ì„ ì™„í™”í•˜ì—¬ ì¬ê²€ìƒ‰...")
            # ì¡°ê±´ ì™„í™” ë¡œì§...
        
        selected_docs = filtered_docs[:5]
        
        print(f"âœ… í•„í„°ë§ëœ ê²€ìƒ‰ ì™„ë£Œ: {len(selected_docs)}ê±´")
        for i, doc in enumerate(selected_docs):
            date = doc.metadata.get('effective_date', 'N/A')
            title = doc.metadata.get('title', '')[:50]
            food_type = doc.metadata.get('ont_food_type', 'N/A')
            contaminant = doc.metadata.get('ont_contaminant', 'N/A')
            print(f"  {i+1}. {date} | {food_type} | {contaminant} | {title}...")
        
        # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context_parts = []
        for doc in selected_docs:
            content_with_meta = f"{doc.page_content}\nSource URL: {doc.metadata.get('url', 'N/A')}"
            context_parts.append(content_with_meta)
        
        context = "\n\n---\n\n".join(context_parts)
        
        return {
            **state,
            "recall_context": context,
            "recall_documents": selected_docs
        }
        
    except Exception as e:
        print(f"í•„í„°ë§ëœ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
        return recall_search_node(state)


def google_news_search_node(state: RecallState) -> RecallState:
    """êµ¬ê¸€ ë‰´ìŠ¤ì—ì„œ ë¦¬ì½œ ì •ë³´ë¥¼ ê²€ìƒ‰"""
    
    try:
        # ë‰´ìŠ¤ ê²€ìƒ‰ ì „ìš© í‚¤ì›Œë“œ ì¶”ì¶œ
        clean_keywords = extract_question_keywords(state["question"]) # "ë§Œë‘ ë¦¬ì½œ ì‚¬ë¡€" â†’ "ë§Œë‘"
        
        print(f"ğŸ“° êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘: '{clean_keywords}' (ì›ë³¸: '{state['question']}')")
        
        # ë‰´ìŠ¤ ê²€ìƒ‰ ë° ë³¸ë¬¸ ì¶”ì¶œ
        news_results = search_and_extract_news_with_fallback(clean_keywords, max_results=3)
        
        if news_results:
            # ë‰´ìŠ¤ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            news_context = format_news_for_context(news_results)
            print(f"âœ… êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ ì™„ë£Œ: {len(news_results)}ê±´")
            
            return {
                **state,
                "recall_context": "",   # ì™„ì „ ì´ˆê¸°í™”
                "recall_documents": [], # ì™„ì „ ì´ˆê¸°í™”
                "news_context": news_context,
                "news_documents": news_results
            }
        else:
            print("âŒ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            # ë‰´ìŠ¤ë„ ì—†ì„ ë•Œ recall ë°ì´í„°ë„ ì™„ì „ ì´ˆê¸°í™”
            return {
                **state,
                "recall_context": "",   # ì´ˆê¸°í™”
                "recall_documents": [], # ì´ˆê¸°í™”
                "news_context": "",
                "news_documents": []
            }
            
    except Exception as e:
        print(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ì‹œì—ë„ recall ë°í‹°í„° ì´ˆê¸°í™”
        return {
            **state,
            "recall_context": "",   # ì´ˆê¸°í™”
            "recall_documents": [], # ì´ˆê¸°í™”
            "news_context": "",
            "news_documents": []
        }


## ğŸ†• ìƒˆë¡œìš´ ì§€ëŠ¥í˜• ë¼ìš°í„°: í•„í„°ë§ -> ìˆ˜ì¹˜í˜• -> ê¸°ë³¸ ê²€ìƒ‰ ìˆœì„œ
def enhanced_intelligent_router(state: RecallState) -> str:
    """í–¥ìƒëœ ì§€ëŠ¥í˜• ë¼ìš°í„° - í•„í„° ì¡°ê±´ ì €ì¥ ë””ë²„ê¹…"""
    
    # ğŸ”§ 1ìˆœìœ„: ë…¼ë¦¬í˜• + ìˆ˜ì¹˜í˜• ì§ˆë¬¸ ì²˜ë¦¬ (í•„í„°ë§ë³´ë‹¤ ë¨¼ì €!)
    if recall_vectorstore:
        # ë…¼ë¦¬í˜• ì§ˆë¬¸ ì²´í¬ (ìµœìš°ì„ )
        logical_processor = LogicalQueryProcessor(recall_vectorstore)
        if logical_processor.is_logical_question(state["question"]):
            print("ğŸ§  1ìˆœìœ„: LLMì´ ë…¼ë¦¬ ì—°ì‚° ì§ˆë¬¸ìœ¼ë¡œ ë¶„ë¥˜ - ë…¼ë¦¬ ë¶„ì„ ìˆ˜í–‰")
            return "logical_analysis"
        
        # ìˆ˜ì¹˜í˜• ì§ˆë¬¸ ì²´í¬ (ë‘ë²ˆì§¸)
        processor = NumericalQueryProcessor(recall_vectorstore)
        if processor.is_numerical_question(state["question"]):
            print("ğŸ§  1ìˆœìœ„: LLMì´ ìˆ˜ì¹˜í˜• ì§ˆë¬¸ìœ¼ë¡œ ë¶„ë¥˜ - ìˆ˜ì¹˜ ë¶„ì„ ìˆ˜í–‰")
            return "numerical_analysis"
    
    # ğŸ”§ 2ìˆœìœ„: í•„í„°ë§ ì§ˆë¬¸ ì²˜ë¦¬ (ë…¼ë¦¬/ìˆ˜ì¹˜í˜• ë‹¤ìŒìœ¼ë¡œ)
    if recall_vectorstore:
        extractor = FilterCriteriaExtractor(recall_vectorstore)
        filter_result = extractor.extract_filter_conditions(state["question"])
        
        detected_filters = filter_result.get("filters_detected", False)
        confidence = filter_result.get("confidence", 0)
        conditions = filter_result.get("conditions", {})
        
        if detected_filters and confidence > 0.6 and conditions:
            print(f"ğŸ¯ 2ìˆœìœ„: í•„í„°ë§ ì§ˆë¬¸ ê°ì§€ (ì‹ ë¢°ë„: {confidence:.2f}, ì¡°ê±´: {conditions})")
            
            # ğŸ†• í•„í„° ì¡°ê±´ ì €ì¥ ë° ë””ë²„ê¹…
            state["filter_conditions"] = conditions
            print(f"ğŸ”§ stateì— ì €ì¥ëœ í•„í„° ì¡°ê±´: {state.get('filter_conditions', 'NOT_FOUND')}")
            
            return "filtered_search"
        else:
            print(f"âš ï¸ í•„í„°ë§ ì¡°ê±´ ë¶ˆì™„ì „ (ì‹ ë¢°ë„: {confidence:.2f}, ì¡°ê±´: {conditions})")
    
    # ë‚˜ë¨¸ì§€ ê¸°ì¡´ ë¡œì§...
    if not is_recall_related_question(state["question"]):
        print("ğŸ“ 3ìˆœìœ„: ì¼ë°˜ ì§ˆë¬¸ - ë‹µë³€ ìƒì„±ìœ¼ë¡œ ì§í–‰")
        return "generate_answer"
    
    # ê¸°ì¡´ ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ íŒë‹¨ ë¡œì§...
    recall_docs = state.get("recall_documents", [])
    recall_count = len(recall_docs)
    
    print(f"ğŸ” ë²¡í„°DB ê²€ìƒ‰ ê²°ê³¼: {recall_count}ê±´")
    
    if recall_count >= 1:
        # ğŸ”§ ê°œì„ ëœ ë§¤ì¹­ ë¡œì§ - ë” ì—„ê²©í•œ ê¸°ì¤€ ì ìš©
        question_en = state.get("question_en", "").lower()
        question_kr = state["question"].lower()
        
        # ğŸ†• í™•ì¥ëœ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
        english_stopwords = ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 
                           'by', 'from', 'about', 'into', 'through', 'during', 'before', 'after', 'above', 
                           'below', 'up', 'down', 'out', 'off', 'over', 'under', 'again', 'further', 'then', 
                           'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 
                           'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 
                           'only', 'own', 'same', 'so', 'than', 'too', 'very', 'can', 'will', 'just', 
                           'should', 'now', 'please', 'tell', 'me', 'case', 'cases']
        korean_stopwords = ['ë¦¬ì½œ', 'ì‚¬ë¡€', 'ê´€ë ¨', 'ìˆë‚˜ìš”', 'ì–´ë–¤', 'ë¬´ì—‡', 'ì•Œë ¤', 'ì£¼ì„¸ìš”', 'í•´ì£¼ì„¸ìš”']
        
        # ì–‘ìª½ ì–¸ì–´ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¶ˆìš©ì–´ ì œê±°
        search_terms = []
        if question_en:
            en_terms = [term for term in question_en.split() 
                       if len(term) > 2 and term not in english_stopwords]
            search_terms.extend(en_terms)
        
        kr_terms = [term for term in extract_question_keywords(question_kr).lower().split()
                   if len(term) > 1 and term not in korean_stopwords]
        search_terms.extend(kr_terms)
        
        # ğŸ†• ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œë§Œ ì¶”ì¶œ (ì œí’ˆëª…, ë¸Œëœë“œëª…, ì˜¤ì—¼ë¬¼ì§ˆ ë“±)
        meaningful_terms = [term for term in search_terms if len(term) > 2]
        
        print(f"ğŸ” ì •ì œëœ ê²€ìƒ‰ í‚¤ì›Œë“œ: {meaningful_terms}")
        
        relevant_count = 0
        high_quality_matches = 0  # ğŸ†• ê³ í’ˆì§ˆ ë§¤ì¹­ ì¹´ìš´í„°
        
        for i, doc in enumerate(recall_docs):
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            ont_food = doc.metadata.get('ont_food', '').lower()
            ont_food_type = doc.metadata.get('ont_food_type', '').lower()
            ont_recall_reason = doc.metadata.get('ont_recall_reason', '').lower()
            ont_contaminant = doc.metadata.get('ont_contaminant', '').lower()
            
            # ğŸ†• ë©”íƒ€ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
            metadata_quality = sum([1 for field in [ont_food, ont_food_type, ont_recall_reason, ont_contaminant] 
                                  if field and field.strip()])
            
            # ì œëª©ê³¼ ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰
            title = doc.metadata.get('title', '').lower()
            content_preview = doc.page_content[:500].lower()
            
            # ğŸ†• ê³ í’ˆì§ˆ ë§¤ì¹­ ì¡°ê±´: ë©”íƒ€ë°ì´í„°ë‚˜ ì œëª©ì—ì„œ ì§ì ‘ ë§¤ì¹­
            high_quality_matches_found = []
            standard_matches_found = []
            
            for term in meaningful_terms:
                # ê³ í’ˆì§ˆ ë§¤ì¹­: ë©”íƒ€ë°ì´í„° ì§ì ‘ ë§¤ì¹­
                if (term in ont_food or term in ont_food_type or 
                    term in ont_contaminant or term in title[:100]):
                    high_quality_matches_found.append(term)
                # ì¼ë°˜ ë§¤ì¹­: ë‚´ìš©ì—ì„œ ë§¤ì¹­  
                elif term in content_preview:
                    standard_matches_found.append(term)
            
            # ğŸ†• ë§¤ì¹­ í’ˆì§ˆ í‰ê°€
            total_matches = high_quality_matches_found + standard_matches_found
            is_high_quality = len(high_quality_matches_found) > 0 and metadata_quality >= 2
            is_standard_relevant = len(total_matches) >= 2  # ìµœì†Œ 2ê°œ í‚¤ì›Œë“œ ë§¤ì¹­ í•„ìš”
            
            if is_high_quality:
                relevant_count += 1
                high_quality_matches += 1
                print(f"    âœ… ê³ í’ˆì§ˆ ë§¤ì¹­ {i+1}: {title[:50]}... (ë©”íƒ€ë°ì´í„° ë§¤ì¹­: {high_quality_matches_found})")
                print(f"        ë©”íƒ€ë°ì´í„°: food={ont_food}, type={ont_food_type}, í’ˆì§ˆì ìˆ˜={metadata_quality}")
            elif is_standard_relevant:
                relevant_count += 1
                print(f"    âœ… í‘œì¤€ ë§¤ì¹­ {i+1}: {title[:50]}... (ë§¤ì¹­: {total_matches})")
                print(f"        ë©”íƒ€ë°ì´í„°: food={ont_food}, type={ont_food_type}, í’ˆì§ˆì ìˆ˜={metadata_quality}")
            else:
                print(f"    âŒ ë§¤ì¹­ ì‹¤íŒ¨ {i+1}: {title[:50]}... (ë§¤ì¹­: {total_matches}, í’ˆì§ˆì ìˆ˜={metadata_quality})")
        
        print(f"ğŸ¯ ë§¤ì¹­ ê²°ê³¼: ê³ í’ˆì§ˆ={high_quality_matches}ê±´, ì „ì²´={relevant_count}/{recall_count}ê±´")
        
        # ğŸ†• ë” ì—„ê²©í•œ í†µê³¼ ê¸°ì¤€
        if high_quality_matches >= 1 or relevant_count >= 2:
            print("ğŸ“‹ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê´€ë ¨ ë¬¸ì„œ ë°œê²¬ - ë‹µë³€ ìƒì„±ìœ¼ë¡œ ì§„í–‰")
            return "generate_answer"
        else:
            print("ğŸ“° ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ - êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ ìˆ˜í–‰")
            return "google_search"
    else:
        print("ğŸ“° ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ - êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ ìˆ˜í–‰")
        return "google_search"


def numerical_analysis_node(state: RecallState) -> RecallState:
    """ğŸ”§ ëª¨ë“ˆí™”ëœ ìˆ˜ì¹˜í˜• ë¶„ì„ ë…¸ë“œ"""
    if not recall_vectorstore:
        return {
            **state,
            "final_answer": "ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ìˆ˜ì¹˜ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }
    
    try:
        processor = NumericalQueryProcessor(recall_vectorstore)
        result = processor.process_numerical_query(state["question"])
        
        if 'error' in result:
            return {
                **state,
                "final_answer": f"ìˆ˜ì¹˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {result['error']}"
            }
        
        # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.1)
        # ğŸ”§ ëª¨ë“ˆí™”ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        prompt = PromptTemplate.from_template(RecallPrompts.NUMERICAL_ANSWER)
        chain = prompt | llm | StrOutputParser()
        
        # ê²°ê³¼ í¬ë§·íŒ…
        if result['type'] == 'ranking':
            formatted_result = "\n".join([f"{i+1}. {item['name']}: {item['count']}ê±´" 
                                        for i, item in enumerate(result['result'])])
        elif result['type'] == 'comparison':
            comp_result = result['result']
            formatted_result = f"ìµœê·¼ í‰ê· : {comp_result['recent_avg']}ê±´/ì›”, ì´ì „ í‰ê· : {comp_result['earlier_avg']}ê±´/ì›”, ì¶”ì„¸: {comp_result['trend']}"
        else:
            formatted_result = str(result['result'])
        
        answer = chain.invoke({
            "question": state["question"],
            "analysis_type": result['type'],
            "result": formatted_result,
            "description": result['description']
        })
        
        final_answer = f"{answer}\n\nğŸ“Š ì •ë³´ ì¶œì²˜: FDA ê³µì‹ ë°ì´í„°ë² ì´ìŠ¤ LLM ê¸°ë°˜ í†µê³„ ë¶„ì„"
        
        return {
            **state,
            "final_answer": final_answer
        }
        
    except Exception as e:
        return {
            **state,
            "final_answer": f"ìˆ˜ì¹˜í˜• ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}"
        }
    
def logical_analysis_node(state: RecallState) -> RecallState:
    """ğŸ”§ LLM ê¸°ë°˜ ë…¼ë¦¬í˜• ë¶„ì„ ë…¸ë“œ (ì™„ì „ ì¬ì‘ì„±)"""
    if not recall_vectorstore:
        return {
            **state,
            "final_answer": "ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ë…¼ë¦¬ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }
    
    try:
        processor = LogicalQueryProcessor(recall_vectorstore)
        result = processor.process_logical_query(state["question"])
        
        if 'error' in result:
            return {
                **state,
                "final_answer": f"ë…¼ë¦¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {result['error']}"
            }
        
        # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.1)
        prompt = PromptTemplate.from_template(RecallPrompts.LOGICAL_ANSWER)
        chain = prompt | llm | StrOutputParser()
        
        # ê²°ê³¼ í¬ë§·íŒ… (LLM ê¸°ë°˜ ê²°ê³¼ì— ë§ê²Œ ìˆ˜ì •)
        operation = result.get('operation', '')
        if operation == 'exclude':
            res = result['result']
            formatted_result = f"ì „ì²´ {res['total_before']}ê±´ â†’ ì œì™¸ {res['excluded_count']}ê±´ â†’ ìµœì¢… {res['final_count']}ê±´"
        elif operation in ['compare', 'temporal']:
            # ğŸ†• ì‹¤ì œ ì›ì¸ëª…ì„ í¬í•¨í•œ ìƒì„¸ ì •ë³´ ì „ë‹¬
            comparison_data = {}
            for subject, data in result['result'].items():
                if isinstance(data, dict):
                    comparison_data[subject] = {
                        'total_count': data.get('total_count', 0),
                        'top_reasons': data.get('top_reasons', {}),
                        'top_allergens': data.get('top_allergens', {}),
                        'top_contaminants': data.get('top_contaminants', {}),
                        'top_food_types': data.get('top_food_types', {})
                    }
            
            # JSON í˜•íƒœë¡œ ì „ë‹¬í•˜ì—¬ LLMì´ íŒŒì‹±í•  ìˆ˜ ìˆë„ë¡
            formatted_result = json.dumps(comparison_data, ensure_ascii=False, indent=2)
        elif operation == 'conditional':
            res = result['result']
            formatted_result = f"ì´ {res['total_count']}ê±´"
        else:
            formatted_result = str(result.get('result', ''))
        
        # ê´€ë ¨ ë§í¬ëŠ” resultì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
        related_links = "\n".join(result.get('example_links', []))
        if not related_links:
            related_links = "ê´€ë ¨ ë¦¬ì½œ ì‚¬ë¡€ ë§í¬ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        answer = chain.invoke({
            "question": state["question"],
            "operation": operation,
            "result": formatted_result,
            "description": result['description'],
            "related_links": related_links
        })
        
        final_answer = f"{answer}\n\nğŸ“Š ì •ë³´ ì¶œì²˜: FDA ê³µì‹ ë°ì´í„°ë² ì´ìŠ¤"
        
        return {
            **state,
            "final_answer": final_answer
        }
        
    except Exception as e:
        return {
            **state,
            "final_answer": f"ë…¼ë¦¬í˜• ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}"
        }


def extract_related_links(result, recall_docs):
    """ë…¼ë¦¬ ì—°ì‚° ê²°ê³¼ì—ì„œ ê´€ë ¨ ë¦¬ì½œ ì‚¬ë¡€ ë§í¬ ì¶”ì¶œ (ê°œì„  ë²„ì „)"""
    links = []
    
    try:
        # operation íƒ€ì…ì— ë”°ë¼ ë§í¬ ì¶”ì¶œ ë°©ì‹ ë‹¤ë¥´ê²Œ ì²˜ë¦¬
        operation = result.get('operation', '')
        
        if operation == 'exclude':
            # ì œì™¸ ì—°ì‚°: ìµœì¢… ê²°ê³¼ ë°ì´í„°ì˜ ë§í¬
            example_links = result.get('example_links', [])
            if example_links:
                links.extend(example_links[:5])  # ìµœëŒ€ 5ê°œ
                
        elif operation == 'compare':
            # ë¹„êµ ì—°ì‚°: ê° ë¹„êµ ëŒ€ìƒë³„ ë§í¬
            example_links = result.get('example_links', {})
            for subject, subject_links in example_links.items():
                if subject_links:
                    links.append(f"ğŸ“Š {subject} ê´€ë ¨:")
                    links.extend(subject_links[:3])  # ê° ëŒ€ìƒë³„ ìµœëŒ€ 3ê°œ
                    links.append("")  # êµ¬ë¶„ìš© ë¹ˆ ì¤„
                    
        elif operation == 'temporal':
            # ì‹œê°„ ë¹„êµ: ê° ê¸°ê°„ë³„ ë§í¬
            example_links = result.get('example_links', {})
            for period, period_links in example_links.items():
                if period_links:
                    links.append(f"ğŸ“… {period} ê´€ë ¨:")
                    links.extend(period_links[:3])  # ê° ê¸°ê°„ë³„ ìµœëŒ€ 3ê°œ
                    links.append("")  # êµ¬ë¶„ìš© ë¹ˆ ì¤„
                    
        elif operation == 'conditional':
            # ì¡°ê±´ë¶€ ì—°ì‚°: ìµœì¢… ê²°ê³¼ ë°ì´í„°ì˜ ë§í¬
            example_links = result.get('example_links', [])
            if example_links:
                links.extend(example_links[:5])  # ìµœëŒ€ 5ê°œ
        
        # ë§í¬ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë¬¸ì„œì—ì„œ ì¶”ì¶œ
        if not links:
            for doc in recall_docs[:5]:
                url = doc.metadata.get('url', '')
                title = doc.metadata.get('title', '')
                date = doc.metadata.get('effective_date', '')
                
                if url and title:
                    short_title = title[:50] + "..." if len(title) > 50 else title
                    links.append(f"â€¢ {short_title} ({date})\n  {url}")
        
        # ë§í¬ê°€ ì—¬ì „íˆ ì—†ìœ¼ë©´ ê¸°ë³¸ ë©”ì‹œì§€
        if not links:
            return "ê´€ë ¨ ë¦¬ì½œ ì‚¬ë¡€ ë§í¬ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        return "\n".join(links)
        
    except Exception as e:
        return f"ë§í¬ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}"
    

def _expand_keywords(self, keywords: List[str]) -> List[str]:
    """ğŸ†• ë” ë²”ìš©ì ì¸ í‚¤ì›Œë“œ í™•ì¥"""
    keyword_mapping = {
        # ì„¸ê· ë¥˜
        'ì‚´ëª¨ë„¬ë¼': ['salmonella'],
        'salmonella': ['ì‚´ëª¨ë„¬ë¼'],
        'ë¦¬ìŠ¤í…Œë¦¬ì•„': ['listeria'],
        'listeria': ['ë¦¬ìŠ¤í…Œë¦¬ì•„'],
        'ëŒ€ì¥ê· ': ['e.coli', 'ecoli', 'e coli'],
        'e.coli': ['ëŒ€ì¥ê· ', 'ecoli'],
        'ecoli': ['ëŒ€ì¥ê· ', 'e.coli'],
        
        # ì‹í’ˆ ì¹´í…Œê³ ë¦¬
        'ì„¸ê· ': ['bacterial', 'bacteria', 'contamination'],
        'bacterial': ['ì„¸ê· ', 'bacteria'],
        'ìœ ì œí’ˆ': ['dairy', 'milk', 'cheese', 'yogurt', 'butter', 'cream'],
        'dairy': ['ìœ ì œí’ˆ', 'milk', 'cheese'],
        'ìœ¡ë¥˜': ['meat', 'beef', 'pork', 'chicken'],
        'meat': ['ìœ¡ë¥˜', 'beef', 'pork'],
        'í•´ì‚°ë¬¼': ['seafood', 'fish', 'salmon', 'shrimp'],
        'seafood': ['í•´ì‚°ë¬¼', 'fish'],
        
        # ì•Œë ˆë¥´ê¸° ê´€ë ¨
        'ì•Œë ˆë¥´ê¸°': ['allergy', 'allergen', 'allergic'],
        'allergen': ['ì•Œë ˆë¥´ê¸°', 'allergy'],
        'ë•…ì½©': ['peanut', 'groundnut'],
        'peanut': ['ë•…ì½©'],
        'ê²¬ê³¼ë¥˜': ['nuts', 'tree nuts', 'almonds', 'walnuts'],
        'nuts': ['ê²¬ê³¼ë¥˜']
    }
    
    expanded = set()
    for keyword in keywords:
        keyword_lower = keyword.lower().strip()
        expanded.add(keyword_lower)
        
        if keyword_lower in keyword_mapping:
            expanded.update(keyword_mapping[keyword_lower])
    
    return list(expanded)

def extract_question_keywords(question: str) -> str:
    """ì‚¬ìš©ì ì˜ë„ë¥¼ ë³´ì¡´í•˜ë©´ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œ"""
    
    # êµ­ê°€ ì§€ì‹œì–´ í™•ì¸
    country_indicators = ["êµ­ì‚°", "êµ­ë‚´", "í•œêµ­ì‚°"]
    is_korean_query = any(indicator in question for indicator in country_indicators)
    
    # ì‚¬ì „ ì •ê·œí™” (í•˜ì§€ë§Œ ì˜ë„ëŠ” ë³´ì¡´)
    normalized = question
    if is_korean_query:
        # êµ­ì‚°/êµ­ë‚´ â†’ í•œêµ­ìœ¼ë¡œ í†µì¼ (ì™„ì „ ì œê±°í•˜ì§€ ì•ŠìŒ)
        normalized = normalized.replace("êµ­ì‚°", "í•œêµ­").replace("êµ­ë‚´", "í•œêµ­").replace("í•œêµ­ì‚°", "í•œêµ­")
    
    # ë¶ˆìš©ì–´ ì œê±°
    stop_words = ["íšŒìˆ˜", "ì‚¬ë¡€", "ìˆë‚˜ìš”", "ê´€ë ¨", "ì–´ë–¤", "ìµœê·¼"]
    
    words = re.findall(r'[ê°€-í£A-Za-z]{2,}', normalized)
    meaningful_words = [word for word in words if word not in stop_words and len(word) >= 2]
    
    # ìµœëŒ€ 2ê°œ í‚¤ì›Œë“œ (í•œêµ­ + ì œí’ˆëª…)
    result = " ".join(meaningful_words[:2])
    
    print(f"ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ (ì˜ë„ ë³´ì¡´): '{question}' â†’ '{result}'")
    return result if result else question

def answer_generation_node(state: RecallState) -> RecallState:
    """ê²€ìƒ‰ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±"""
    
    is_recall_question = is_recall_related_question(state["question"])
    
    if not is_recall_question:
        try:
            llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.3)
            # ğŸ”§ ëª¨ë“ˆí™”ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            prompt = PromptTemplate.from_template(RecallPrompts.GENERAL_QUESTION)
            chain = prompt | llm | StrOutputParser()
            
            answer = chain.invoke({"question": state["question"]})
            final_answer = f"{answer}\n\nğŸ’¡ ì¼ë°˜ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬ë¨"
            
            return {
                **state,
                "final_answer": final_answer
            }
            
        except Exception as e:
            return {
                **state,
                "final_answer": f"ì¼ë°˜ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}"
            }
    
    # ë¦¬ì½œ ê´€ë ¨ ì§ˆë¬¸ ì²˜ë¦¬
    try:
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.1)
        
        recall_context = state.get("recall_context", "").strip()
        news_context = state.get("news_context", "").strip()

        print(f"ğŸ” recall_context ê¸¸ì´: {len(recall_context)}")
        print(f"ğŸ” news_context ê¸¸ì´: {len(news_context)}")
        
        # ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì„ íƒ
        if news_context:
            print("ğŸ“° ë‰´ìŠ¤ ë°ì´í„° ê¸°ë°˜ ë‹µë³€ ì„ íƒ")
            # ğŸ”§ ëª¨ë“ˆí™”ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            prompt = PromptTemplate.from_template(RecallPrompts.NEWS_ANSWER)
            context = news_context
            source_type = "ìµœì‹  ë‰´ìŠ¤"
        elif recall_context:
            print("ğŸ“‹ FDA ë°ì´í„° ê¸°ë°˜ ë‹µë³€ ì„ íƒ")
            # ğŸ”§ ëª¨ë“ˆí™”ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            prompt = PromptTemplate.from_template(RecallPrompts.RECALL_ANSWER)
            context = recall_context
            source_type = "FDA ê³µì‹ ë°ì´í„°"
        else:
            return {
                **state,
                "final_answer": "í˜„ì¬ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ í•´ë‹¹ ë¦¬ì½œ ì‚¬ë¡€ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }
        
        chain = prompt | llm | StrOutputParser()
        
        answer = chain.invoke({
            "question": state["question"],
            "recall_context": context if recall_context else "",
            "news_context": context if news_context else ""
        })
        
        # ê²€ìƒ‰ ì •ë³´ ì¶”ê°€
        search_info = f"\n\nğŸ“‹ ì •ë³´ ì¶œì²˜: {source_type}"
        
        if recall_context:
            recall_docs = state.get("recall_documents", [])
            if recall_docs:
                realtime_count = len([doc for doc in recall_docs 
                                   if doc.metadata.get("source") == "realtime_crawl"])
                search_info += f" (ì´ {len(recall_docs)}ê±´"
                if realtime_count > 0:
                    search_info += f", âš¡ì‹¤ì‹œê°„: {realtime_count}ê±´"
                search_info += ")"
        elif news_context:
            news_docs = state.get("news_documents", [])
            search_info += f" (ë‰´ìŠ¤ {len(news_docs)}ê±´)"
        
        final_answer = f"{answer}{search_info}"
        
        return {
            **state,
            "final_answer": final_answer
        }
        
    except Exception as e:
        return {
            **state,
            "final_answer": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}"
        }

def update_history_node(state: RecallState) -> RecallState:
    """ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ ì—…ë°ì´íŠ¸"""
    try:
        current_history = state.get("chat_history", [])
        
        updated_history = current_history.copy()
        updated_history.append(HumanMessage(content=state["question"]))
        updated_history.append(AIMessage(content=state["final_answer"]))
        
        # íˆìŠ¤í† ë¦¬ ê¸¸ì´ ì œí•œ (ìµœëŒ€ 8ê°œ ë©”ì‹œì§€)
        if len(updated_history) > 8:
            updated_history = updated_history[-8:]
        
        return {
            **state,
            "chat_history": updated_history
        }
        
    except Exception as e:
        print(f"íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
        return state

## LLM ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ì„¤ì •
def setup_llm_enhanced_workflow():
    """ê¸°ì¡´ ì›Œí¬í”Œë¡œìš°ì— LLM ê¸°ë°˜ ìˆ˜ì¹˜í˜• ì²˜ë¦¬ ë…¸ë“œ ì¶”ê°€ - ChromaDB ì „ìš©"""
    
    # ChromaDB ì„¤ì • í™•ì¸
    if recall_vectorstore:
        print("ğŸ”§ ChromaDB ì„¤ì • í™•ì¸ ì¤‘...")
        verify_chromadb_setup(recall_vectorstore)
    
    workflow = StateGraph(RecallState)
    
    # ê¸°ì¡´ ë…¸ë“œ
    workflow.add_node("translate", translation_node)
    workflow.add_node("recall_search", recall_search_node)
    workflow.add_node("google_search", google_news_search_node)
    workflow.add_node("generate_answer", answer_generation_node)
    workflow.add_node("update_history", update_history_node)
    workflow.add_node("numerical_analysis", numerical_analysis_node) # LLM ê¸°ë°˜ ìˆ˜ì¹˜í˜• ë¶„ì„ ë…¸ë“œ
    workflow.add_node("logical_analysis", logical_analysis_node)  # ğŸ†• ë…¼ë¦¬ ì—°ì‚° ë…¸ë“œ
    workflow.add_node("filtered_search", filtered_search_node)

    # ê¸°ì¡´ ì—£ì§€
    workflow.add_edge(START, "translate")
    workflow.add_edge("translate", "recall_search")
    
    # ì¡°ê±´ë¶€ ì—£ì§€
    workflow.add_conditional_edges("recall_search", enhanced_intelligent_router, {
         "filtered_search": "filtered_search",   
        "logical_analysis": "logical_analysis",
        "numerical_analysis": "numerical_analysis",  # ğŸ†• LLM ê¸°ë°˜ ìˆ˜ì¹˜ ë¶„ì„ ê²½ë¡œ ì¶”ê°€
        "google_search": "google_search",
        "generate_answer": "generate_answer"
    })

    workflow.add_edge("filtered_search", "generate_answer")  # ğŸ†• í•„í„°ë§ ê²€ìƒ‰ â†’ ë‹µë³€ ìƒì„±
    workflow.add_edge("google_search", "generate_answer")
    workflow.add_edge("logical_analysis", "update_history") 
    workflow.add_edge("numerical_analysis", "update_history")
    workflow.add_edge("generate_answer", "update_history")
    workflow.add_edge("update_history", END)

    # ê·¸ë˜í”„ ì»´íŒŒì¼
    return workflow.compile()

recall_graph = setup_llm_enhanced_workflow() # ğŸ†• ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”

def ask_recall_question(question: str, chat_history: List = None) -> Dict[str, Any]:
    """ë¦¬ì½œ ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""
    if chat_history is None:
        chat_history = []
    
    try:
        result = recall_graph.invoke({
            "question": question,
            "question_en": "",  # ë²ˆì—­ ë…¸ë“œì—ì„œ ì±„ì›Œì§
            "recall_context": "",
            "recall_documents": [],
            "final_answer": "",
            "chat_history": chat_history,
            "news_context": "",
            "news_documents": [],
            "filter_conditions": {} 
        })
        
        return {
            "answer": result["final_answer"],
            "recall_documents": result["recall_documents"],
            "chat_history": result["chat_history"]
        }
        
    except Exception as e:
        return {
            "answer": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}",
            "recall_documents": [],
            "chat_history": chat_history
        }