# utils/google_crawler.py
"""
êµ¬ê¸€ ë‰´ìŠ¤ RSS ë° Selenium ê¸°ë°˜ ë¦¬ì½œ ì •ë³´ ê²€ìƒ‰ ëª¨ë“ˆ
"""
import feedparser
import time
import re
from datetime import datetime
from typing import List, Dict
from urllib.parse import quote_plus

# Selenium ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup


def search_google_news_rss(keyword: str, max_results: int = 5, days_back: int = 30) -> List[Dict]:
    """ì´ ì½”ë“œëŠ” êµ¬ê¸€ ë‰´ìŠ¤ RSSì—ì„œ ë¦¬ì½œ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤"""
    try:
        search_strategies = [
            f"{keyword} ë¦¬ì½œ",           
            f"{keyword} ë¯¸êµ­ ë¦¬ì½œ",          
            f"{keyword} recall USA",        
            f"{keyword} ì œí’ˆ íšŒìˆ˜"           
        ]
        
        all_results = []
        
        for i, search_query in enumerate(search_strategies):
            print(f"ğŸ” ê²€ìƒ‰ ì „ëµ {i+1}: '{search_query}'")
            
            encoded_query = quote_plus(search_query)
            rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko"
            
            feed = feedparser.parse(rss_url)
            print(f"   RSS ê²°ê³¼: {len(feed.entries)}ê±´")
            
            if not feed.entries:
                continue
                
            strategy_results = []
            for entry in feed.entries[:max_results]:
                try:
                    pub_date = None
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_date = datetime(*entry.published_parsed[:6])
                    
                    title = entry.title if hasattr(entry, 'title') else 'ì œëª© ì—†ìŒ'
                    link = entry.link if hasattr(entry, 'link') else ''
                    summary = entry.summary if hasattr(entry, 'summary') else ''
                    source = entry.source.title if hasattr(entry, 'source') and hasattr(entry.source, 'title') else 'Unknown'
                    
                    is_fda_related = any(term in (title + summary).lower() for term in ['fda', 'ë¯¸êµ­', 'usa', 'america', 'recall', 'ë¦¬ì½œ'])
                    is_recall_relevant = is_recall_related_text(title + " " + summary)
                    
                    if is_fda_related or is_recall_relevant:
                        strategy_results.append({
                            'title': title, 'link': link, 'summary': summary,
                            'published': pub_date.strftime('%Y-%m-%d %H:%M') if pub_date else 'Unknown',
                            'source': source, 'content': '', 'search_strategy': i+1,
                            'is_fda_related': is_fda_related, 'is_recall_related': is_recall_relevant
                        })
                except Exception:
                    continue
            
            all_results.extend(strategy_results)
            if len(all_results) >= max_results:
                break
        
        def sort_priority(item):
            fda_score = 10 if item['is_fda_related'] else 0
            recall_score = 5 if item['is_recall_related'] else 0
            strategy_score = 6 - item['search_strategy']
            return fda_score + recall_score + strategy_score
        
        all_results.sort(key=sort_priority, reverse=True)
        
        unique_results = []
        seen_titles = set()
        for result in all_results:
            if result['title'] not in seen_titles:
                unique_results.append(result)
                seen_titles.add(result['title'])
        
        return unique_results[:max_results]
        
    except Exception:
        return []
    
def search_bing_news_rss(keyword: str, max_results: int = 5) -> List[Dict]:
    """ì´ ì½”ë“œëŠ” Bing ë‰´ìŠ¤ RSSì—ì„œ ë¦¬ì½œ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤"""
    try:
        search_strategies = [
            f"{keyword} FDA ë¦¬ì½œ",           
            f"{keyword} ë¯¸êµ­ ë¦¬ì½œ",          
            f"{keyword} recall USA",        
            f"{keyword} FDA recall"
        ]

        print(f"ğŸ“ ê²€ìƒ‰ í‚¤ì›Œë“œ: '{keyword}'")
        print(f"ğŸ“ ìƒì„±ëœ ì „ëµ: {search_strategies}")
        
        all_results = []
        
        for i, search_query in enumerate(search_strategies):
            print(f"ğŸ” Bing ê²€ìƒ‰ ì „ëµ {i+1}: '{search_query}'")
            
            encoded_query = quote_plus(search_query)
            rss_url = f"https://www.bing.com/news/search?q={encoded_query}&format=RSS"
            
            # RSS íŒŒì‹±
            feed = feedparser.parse(rss_url)
            print(f"   Bing RSS ê²°ê³¼: {len(feed.entries)}ê±´")
            
            if not feed.entries:
                continue
                
            strategy_results = []
            for entry in feed.entries[:max_results]:
                try:
                    # Bing RSS êµ¬ì¡°ì— ë§ê²Œ íŒŒì‹±
                    pub_date = None
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_date = datetime(*entry.published_parsed[:6])
                    
                    title = entry.title if hasattr(entry, 'title') else 'ì œëª© ì—†ìŒ'
                    link = entry.link if hasattr(entry, 'link') else ''
                    summary = entry.summary if hasattr(entry, 'summary') else ''
                    
                    # Bingì€ source êµ¬ì¡°ê°€ ë‹¤ë¦„
                    source = "Bing News"
                    
                    is_fda_related = any(term in (title + summary).lower() 
                                       for term in ['fda', 'ë¯¸êµ­', 'usa', 'america', 'recall', 'ë¦¬ì½œ'])
                    is_recall_relevant = is_recall_related_text(title + " " + summary)
                    
                    if is_fda_related or is_recall_relevant:
                        strategy_results.append({
                            'title': title,
                            'link': link, 
                            'summary': summary,
                            'published': pub_date.strftime('%Y-%m-%d %H:%M') if pub_date else 'Unknown',
                            'source': source,
                            'content': '',
                            'search_strategy': i+1,
                            'is_fda_related': is_fda_related,
                            'is_recall_related': is_recall_relevant,
                            'search_engine': 'bing'  # êµ¬ë¶„ìš©
                        })
                except Exception:
                    continue
            
            all_results.extend(strategy_results)
            if len(all_results) >= max_results:
                break
        
        # ìš°ì„ ìˆœìœ„ ì •ë ¬ (ê¸°ì¡´ê³¼ ë™ì¼)
        def sort_priority(item):
            fda_score = 10 if item['is_fda_related'] else 0
            recall_score = 5 if item['is_recall_related'] else 0
            strategy_score = 6 - item['search_strategy']
            return fda_score + recall_score + strategy_score
        
        all_results.sort(key=sort_priority, reverse=True)
        
        # ì¤‘ë³µ ì œê±°
        unique_results = []
        seen_titles = set()
        for result in all_results:
            if result['title'] not in seen_titles:
                unique_results.append(result)
                seen_titles.add(result['title'])
        
        return unique_results[:max_results]
        
    except Exception as e:
        print(f"Bing RSS ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []
    
def search_and_extract_news_with_fallback(keyword: str, max_results: int = 3) -> List[Dict]:
    """ì´ ì½”ë“œëŠ” ë‹¨ìˆœí•˜ê³  ì •í™•í•œ ë‹¨ì¼ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤ (bing ê²€ìƒ‰+ google ë°±ì—… í¬ë¡¤ë§)"""
    
    # ğŸ†• í•œêµ­ ì œí’ˆì¸ ê²½ìš° í•µì‹¬ ì œí’ˆëª…ë§Œ ì¶”ì¶œ
    korean_indicators = ["í•œêµ­", "êµ­ì‚°", "êµ­ë‚´", "Korean"]
    
    final_keyword = keyword
    for indicator in korean_indicators:
        final_keyword = final_keyword.replace(indicator, "").strip()
    
    # ğŸ†• ìµœì¢… ê²€ìƒ‰ í‚¤ì›Œë“œ ê²°ì •
    search_keyword = final_keyword if (final_keyword and len(final_keyword) > 1) else keyword
    
    print(f"ğŸ” ë‹¨ì¼ ê²€ìƒ‰: '{search_keyword}' (ì›ë³¸: '{keyword}')")
    
    # ğŸ†• Bing ìš°ì„ , Google ë°±ì—… (ê¸°ì¡´ fallback ë¡œì§ ìœ ì§€)
    all_results = []
    
    # 1ìˆœìœ„: Bing RSS ì‹œë„
    try:
        print("ğŸ” Bing ë‰´ìŠ¤ RSS ê²€ìƒ‰ ì‹œë„...")
        bing_results = search_bing_news_rss(search_keyword, max_results)
        
        if bing_results:
            print(f"âœ… Bing ê²€ìƒ‰ ì„±ê³µ: {len(bing_results)}ê±´")
            all_results.extend(bing_results)
        else:
            print("âŒ Bingì—ì„œ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
    except Exception as e:
        print(f"âŒ Bing ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    # 2ìˆœìœ„: ê²°ê³¼ê°€ ì—†ìœ¼ë©´ Google ë°±ì—…
    if not all_results:
        try:
            print("ğŸ”„ Google RSS ë°±ì—… ê²€ìƒ‰ ì‹œë„...")
            google_results = search_google_news_rss(search_keyword, max_results)
            
            if google_results:
                # ê²€ìƒ‰ ì—”ì§„ êµ¬ë¶„ì„ ìœ„í•´ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                for result in google_results:
                    result['search_engine'] = 'google'
                
                all_results.extend(google_results)
                print(f"âœ… Google ë°±ì—… ê²€ìƒ‰ ì„±ê³µ: {len(google_results)}ê±´")
                
        except Exception as e:
            print(f"âŒ Google ë°±ì—… ê²€ìƒ‰ë„ ì‹¤íŒ¨: {e}")
    
    # ğŸ†• ê²°ê³¼ í™•ì¸
    if not all_results:
        print("âŒ ëª¨ë“  ê²€ìƒ‰ ì—”ì§„ì—ì„œ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return []
    
    # ë³¸ë¬¸ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼)
    enriched_results = []
    for i, news_item in enumerate(all_results):
        engine = news_item.get('search_engine', 'bing')
        print(f"({i+1}/{len(all_results)}) [{engine}] ë³¸ë¬¸ ì¶”ì¶œ ì¤‘: {news_item['title'][:50]}...")
        
        content = extract_news_content_selenium(news_item['link'])
        
        if content and len(content) > 50:
            news_item['content'] = content
        else:
            summary_text = BeautifulSoup(news_item.get('summary', ''), 'html.parser').get_text()
            news_item['content'] = summary_text
        
        enriched_results.append(news_item)
        time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
    
    print(f"âœ… ìµœì¢… ê²€ìƒ‰ ì™„ë£Œ: {len(enriched_results)}ê±´")
    return enriched_results


def is_recall_related_text(text: str) -> bool:
    """ì´ ì½”ë“œëŠ” í…ìŠ¤íŠ¸ê°€ ë¦¬ì½œ ê´€ë ¨ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤"""
    recall_keywords = [
        "ë¦¬ì½œ", "íšŒìˆ˜", "recall", "withdrawal", "ì‹í’ˆì•ˆì „", "ì˜¤ì—¼", "contamination",
        "ì„¸ê· ", "bacteria", "ì•ˆì „ê²½ê³ ", "ìœ„í—˜", "ì‹ì¤‘ë…", "ì•Œë ˆë¥´ê¸°", "ë¼ë²¨ë§",
        "ë¬¸ì œ", "ê²°í•¨", "í•˜ì", "ë¶€ì í•©", "íŒë§¤ì¤‘ë‹¨", "ìœ í†µì¤‘ë‹¨", "ë°˜í’ˆ"
    ]
    text_lower = text.lower()
    return any(keyword.lower() in text_lower for keyword in recall_keywords)


# ì „ì—­ ë“œë¼ì´ë²„ ìºì‹œ (ì„¸ì…˜ë‹¹ ì¬ì‚¬ìš©)
_driver_cache = None

def get_optimized_driver():
    """ì´ ì½”ë“œëŠ” ìµœì í™”ëœ Selenium ë“œë¼ì´ë²„ë¥¼ ì¬ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ ì œê³µí•©ë‹ˆë‹¤"""
    global _driver_cache
    
    if _driver_cache is None:
        options = Options()
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--disable-extensions")
        options.add_argument("--disable-plugins")
        options.add_argument("--disable-images")  # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™”
        options.add_argument("--disable-javascript")  # JS ë¹„í™œì„±í™” (ë‰´ìŠ¤ ë³¸ë¬¸ì€ ëŒ€ë¶€ë¶„ ì •ì )
        options.add_argument("--page-load-strategy=eager")  # DOM ë¡œë”©ë§Œ ê¸°ë‹¤ë¦¼
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        
        service = Service(ChromeDriverManager().install())
        _driver_cache = webdriver.Chrome(service=service, options=options)
        _driver_cache.set_page_load_timeout(10)  # íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•
    
    return _driver_cache

def close_driver_cache():
    """ì´ ì½”ë“œëŠ” ìºì‹œëœ ë“œë¼ì´ë²„ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤"""
    global _driver_cache
    if _driver_cache:
        try:
            _driver_cache.quit()
        except:
            pass
        finally:
            _driver_cache = None

def extract_news_content_selenium(url: str) -> str:
    """ì´ ì½”ë“œëŠ” ìµœì í™”ëœ Seleniumìœ¼ë¡œ ë‰´ìŠ¤ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤"""
    try:
        driver = get_optimized_driver()
        
        # í˜ì´ì§€ ì ‘ì† (íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•)
        driver.get(url)
        time.sleep(1)  # ëŒ€ê¸° ì‹œê°„ ë‹¨ì¶• (3ì´ˆ â†’ 1ì´ˆ)

        # í˜ì´ì§€ ì†ŒìŠ¤ë¥¼ BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for script in soup(["script", "style", "nav", "header", "footer", "aside"]):
            script.decompose()
        
        # ë³¸ë¬¸ ì¶”ì¶œ ì„ íƒì (íš¨ìœ¨ì ì¸ ìˆœì„œë¡œ ì¬ë°°ì—´)
        content_selectors = [
            'article',  # ê°€ì¥ ì¼ë°˜ì 
            '.article-content', '.news-content', '.post-content', 
            'div[class*="content"]', 'div[class*="article"]'
        ]
        
        content = ""
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                best_text = max((elem.get_text().strip() for elem in elements), 
                              key=len, default="")
                if len(best_text) > len(content):
                    content = best_text
                if len(content) > 300:  # ì¶©ë¶„í•œ ë‚´ìš©ì´ë©´ ì¤‘ë‹¨
                    break
        
        # p íƒœê·¸ ë°±ì—… ì¶”ì¶œ
        if len(content) < 100:
            paragraphs = [p.get_text().strip() for p in soup.find_all('p') 
                         if len(p.get_text().strip()) > 30]
            content = '\n'.join(paragraphs)
        
        # í›„ì²˜ë¦¬
        if content:
            content = re.sub(r'\s+', ' ', content).strip()[:2000]  # ê¸¸ì´ ì œí•œ ë‹¨ì¶•
        
        print(f"  âœ… ìµœì í™” í¬ë¡¤ë§ ì„±ê³µ: {len(content)}ì")
        return content
        
    except Exception as e:
        print(f"  âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {url} - {e}")
        return ""


def format_news_for_context(news_results: List[Dict]) -> str:
    """ì´ ì½”ë“œëŠ” ë‰´ìŠ¤ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ í˜•íƒœë¡œ í¬ë§·í•©ë‹ˆë‹¤"""
    if not news_results:
        return ""
    
    context_parts = []
    for news in news_results:
        news_context = f"""
ë‰´ìŠ¤ ì œëª©: {news.get('title', '')}
ì¶œì²˜: {news.get('source', 'Unknown')}
ë°œí–‰ì¼: {news.get('published', 'Unknown')}
URL: {news.get('link', '')}

ê¸°ì‚¬ ë‚´ìš©:
{news.get('content', '')}
        """.strip()
        context_parts.append(news_context)
    
    return "\n\n---\n\n".join(context_parts)
